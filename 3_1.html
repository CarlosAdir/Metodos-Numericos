<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<link rel="stylesheet" href="architect.css">
	<link rel="stylesheet" href="custom.css">
	<script id="MathJax-script" async
	src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
	</script>
	<title>MetNum - Regressão linear</title>
</head>
<body>
	<header>
		<h1>Métodos Numéricos</h1>
	</header>

	<nav>
		
		<a href="index.html">Inicio</a>
		<ol>
			<li><a href="1.html">Encontrar raizes 1D</a>
			<ol>
				<li><a href="1_1.html">Bisecção</a></li>
				<li><a href="1_2.html">Ponto Fixo</a></li>
				<li><a href="1_3.html">Newton</a></li>
				<li><a href="1_4.html">Secantes</a></li>
			</ol>
			</li>
			<li><a href="2.html">Interpolação</a>
			<ol>
				<li><a href="2_1.html">Lagrange</a></li>
				<li><a href="2_2.html">Hermite</a></li>
				<li><a href="2_3.html">Spline</a></li>
			</ol>
			</li>
			<li><a href="3.html">Teoria da Aproximação</a>
			<ol>
				<li><a href="3_1.html">Regressão Linear</a></li>
				<li><a href="3_2.html">Minimos quadráticos</a></li>
			</ol>
			</li>
			<li><a href="4.html">Integração Numérica</a>
			<ol>
				<li><a href="4_1.html">Retângulos</a></li>
				<li><a href="4_2.html">Trapézios</a></li>
				<li><a href="4_3.html">Simpson de 1/3</a></li>
				<li><a href="4_4.html">Simpson de 3/8</a></li>
		
			</ol>
			</li>
			<li><a href="5.html">Resolução de sistemas lineares</a>
			<ol>
				<li><a href="5_1.html">Eliminação de Gauss</a></li>
				<li><a href="5_2.html">Decomposição LU</a></li>
				<li><a href="5_3.html">Iterações de Jacobi</a></li>
				<li><a href="5_4.html">Iterações de Gauss-Seidel</a></li>
			</ol>
			</li>
			<li><a href="6.html">Resolução de EDO's de valor inicial</a>
			<ol>
				<li><a href="6_1.html">Euler</a></li>
				<li><a href="6_2.html">Taylor</a></li>
				<li><a href="6_3.html">Runge Kutta</a></li>
			</ol>
			</li>
			<li><a href="7.html">Resolução de EDO's com contorno</a>
			<ol>
				<li><a href="7_1.html">Shooting linear</a></li>
				<li><a href="7_2.html">Shooting para não linear</a></li>
			</ol>
			</li>
		</ol>
	</nav>
	<main>
		 <article>

			<h2>Regressão linear</h2>

			<h3 class="resumo">Resumo</h3>
			<div class="subsec">
				<p>Este método consiste em pegar \(n\) pontos e descobrir a reta(dada por \(y = ax+b\)) que seja a mais próxima possível dos pontos</p>
				<p>Para isso, minimizamos a soma dos quadrados das distâncias. Ou seja, diminuir ao máximo o valor de</p>

				$$\sum_{i=1}^{n}(ax_i + b - y_i)^2$$

				<img src="https://upload.wikimedia.org/wikipedia/commons/4/41/LinearRegression.svg">
			</div>

			<h3 class="teoria">Teoria</h3>
			<div class="subsec">
				<p>Nós temos \(n\) pontos dados, com coordenadas \((x_1, \ y_1), \ \cdots, \ (x_n, \ y_n)\).</p>

				<p>E temos uma reta com coeficientes \(a\) e \(b\) ainda desconhecidos:

				$$r: \ y = ax + b$$

				<p>Daí calculamos a soma dos quadrados das distâncias verticais até a reta:</p>

				$$S(a, \ b) = \sum_{i=1}^{n} \left(ax_i + b - y_i\right)^2$$

				<p>E então, queremos minizar o valor de \(S\)(usando o método dos mínimos quadráticos), e para isso, derivamos parcialmente e igualamos à zero:</p>

				$$\begin{align*}
					\dfrac{\partial S}{\partial a} = 0 \Leftrightarrow & \sum_{i=1}^{n}2x_i\left(ax_i + b - y_i\right) = 0 \\
					\dfrac{\partial S}{\partial b} = 0 \Leftrightarrow & \sum_{i=1}^{n}2\left(ax_i + b - y_i\right) = 0
				\end{align*}$$

				<p>Reorganizando nós  temos um sistema</p>

				$$\begin{align*}
				\begin{cases}
				a\sum x_i^2 + b\sum x_i = \sum x_i y_i \\
				a\sum x_i   + b \cdot n = \sum y_i
				\end{cases}
				\end{align*}$$

				<p>E na forma matricial</p>

				$$\begin{bmatrix}
				\sum x_i^2 & \sum x_i \\
				\sum x_i   &  n
				\end{bmatrix}\cdot \begin{bmatrix}
				a \\ b
				\end{bmatrix} = \begin{bmatrix}
				\sum x_i y_i \\ \sum y_i
				\end{bmatrix}$$

				<p>Que obtemos como solução:</p>

				$$m = n\sum x_i^2 - \left(\sum x_i\right)^2$$
				$$a = \dfrac{n \sum x_i y_i - \sum x_i \sum y_i}{m}$$
				$$b = \dfrac{\sum y_i \sum x_i^2 - \sum x_i \sum x_i y_i}{m}$$			

				<p>Podemos ver pela <a href="https://pt.wikipedia.org/w/index.php?title=Desigualdade_das_m%C3%A9dias&oldid=45503333">desigualdade das médias</a>, o valor de \(m\) é sempre positivo pois:</p>

				$$\sqrt{\dfrac{x_1^2 + \cdots + x_n^2}{n}} \ge \dfrac{x_1 + \cdots + x_n}{n}$$

				<p>Ocorrendo a igualdade se e somente se \(x_0 = x_1 = \cdots = x_n\)</p>
			</div>

			<h3 class="erros empty">Análise de Erros</h3>
			<div class="subsec">
			</div>

			<h3 class="restricoes">Restrições</h3>
			<div class="subsec">
			<ul>
				<li>os \(x_i\) não são todos iguais \(\Rightarrow m \ne 0\)<!-- , ou seja, existe ao menos um \(x_i\) diferente dos demais. --></li>
			</ul>
			</div>

			<h3 class="parametros empty">Parâmetros</h3>
			<div class="subsec">
				<ul>
					<li>A quantidade de pontos: \(n\)</li>
					<li>Os \(n\) pontos: \(\left(x_i, \ y_i\right)\)</li>
				</ul>
			</div>
			
			<h3 class="exemplo empty">Exemplos</h3>
			<div class="subsec">
			</div>

			<h3 class="more empty">Creditos e saiba mais</h3>
			<div class="subsec">
			</div>


			
		</article>
	</main>

	<footer>
		<p>©Copyright 2020 by CarlosAdir. All rights reserved.</p>
    </footer>


</body>
</html>
